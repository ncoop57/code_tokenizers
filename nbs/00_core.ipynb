{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# core\n",
    "\n",
    "> Fill in a module description here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "# | export\n",
    "import code_tokenizers\n",
    "import json\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from tree_sitter import Language, Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class ASTNode:\n",
    "    def __init__(self, node, is_internal, is_builtin, node_types):\n",
    "        self.node = node\n",
    "        self.is_internal = is_internal\n",
    "        self.is_builtin = is_builtin\n",
    "\n",
    "        self.type = node.type\n",
    "        self.parent_type = node.parent.type\n",
    "        self.type_id = node_types.index(self.type)\n",
    "        self.parent_type_id = node_types.index(self.parent_type)\n",
    "\n",
    "    def __str__(self):\n",
    "        if self.type == -1 or self.parent_type == -1:\n",
    "            return \"< N/A >\"\n",
    "        if self.is_internal:\n",
    "            return f\"<{self.parent_type} -> {self.type} (internal)>\"\n",
    "        if self.is_builtin:\n",
    "            return f\"<{self.parent_type} -> {self.type} (builtin)>\"\n",
    "        return f\"<{self.parent_type} -> {self.type}>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def unroll_node_types(\n",
    "    nested_node_types: dict,  # node_types from tree-sitter\n",
    ") -> list:  # list of node types\n",
    "    \"\"\"Unroll nested node types into a flat list of node types. This includes subtypes as well.\"\"\"\n",
    "    node_types = [node_type[\"type\"] for node_type in nested_node_types]\n",
    "    node_subtypes = [\n",
    "        node_subtype[\"type\"]\n",
    "        for node_type in node_types\n",
    "        if \"subtypes\" in node_type\n",
    "        for node_subtype in node_type[\"subtypes\"]\n",
    "    ]\n",
    "    return list(set(node_types + node_subtypes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "# From: https://github.com/github/CodeSearchNet/tree/master/function_parser\n",
    "def traverse(\n",
    "    node,  # tree-sitter node\n",
    "    results,  # list to append results to\n",
    ") -> None:\n",
    "    \"\"\"Traverse in a recursive way, a tree-sitter node and append results to a list.\"\"\"\n",
    "    if node.type == \"string\":\n",
    "        results.append(node)\n",
    "        return\n",
    "    for n in node.children:\n",
    "        traverse(n, results)\n",
    "    if not node.children:\n",
    "        results.append(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def get_token_type(\n",
    "    tok_span: tuple,  # (start, end) position of a token\n",
    "    nodes: list,  # list of tree-sitter nodes\n",
    "    lines: list,  # list of lines in the code\n",
    "    internal_methods: list,  # list of internal methods\n",
    "    acceptable_ast_types: list,  # list of AST types to accept for internal methods\n",
    "    node_types: list,  # list of node types\n",
    ") -> tuple:  # (parent_type, token_type) of the token\n",
    "    \"\"\"Get the parent AST type and token AST type of a token.\"\"\"\n",
    "\n",
    "    def get_node_span(node):\n",
    "        def convert_to_offset(point):\n",
    "            row, column = point\n",
    "            chars_in_rows = sum(map(len, lines[:row])) + row\n",
    "            chars_in_columns = len(lines[row][:column])\n",
    "\n",
    "            offset = chars_in_rows + chars_in_columns\n",
    "            return offset\n",
    "\n",
    "        start_span = convert_to_offset(node.start_point)\n",
    "        end_span = convert_to_offset(node.end_point)\n",
    "        return start_span, end_span\n",
    "\n",
    "    node_spans = [get_node_span(node) for node in nodes]\n",
    "    for i, span in enumerate(node_spans):\n",
    "        if (span[0] <= tok_span[0] and tok_span[0] < span[1]) or (\n",
    "            span[0] < tok_span[1] and tok_span[1] <= span[1]\n",
    "        ):\n",
    "            is_internal = (\n",
    "                nodes[i].text.decode() in internal_methods\n",
    "                and nodes[i].parent.type in acceptable_ast_types\n",
    "            )\n",
    "            is_builtin = (\n",
    "                nodes[i].text.decode() in dir(__builtins__)\n",
    "                and nodes[i].parent.type == \"call\"\n",
    "            )\n",
    "            if not is_internal:\n",
    "                if nodes[i].parent.parent is not None:\n",
    "                    if nodes[i].parent.parent.type in \"call\":\n",
    "                        if (\n",
    "                            nodes[i].parent.parent.named_children[0].text.decode()\n",
    "                            in internal_methods\n",
    "                        ):\n",
    "                            is_internal = True\n",
    "\n",
    "            ast_node = ASTNode(nodes[i], is_internal, is_builtin, node_types)\n",
    "            return ast_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class CodeTokenizer:\n",
    "    \"\"\"A tokenizer for code, which aligns the tokens with the AST nodes.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer,  # transformers tokenizer\n",
    "        parser,  # tree-sitter parser\n",
    "        language,  # tree-sitter language\n",
    "        node_types,  # list of node types\n",
    "        name_or_path,  # name or path of the tokenizer\n",
    "        program_lang,  # programming language of the tokenizer\n",
    "        padding_token,  # whether to add a padding token\n",
    "    ):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.parser = parser\n",
    "        self.language = language\n",
    "        self.node_types = node_types\n",
    "        self.name_or_path = name_or_path\n",
    "        self.program_lang = program_lang\n",
    "        self.padding_token = padding_token\n",
    "\n",
    "        if self.program_lang == \"python\":\n",
    "            self.acceptable_ast_types = [\"call\", \"argument_list\"]\n",
    "\n",
    "    def parse_tree(\n",
    "        self,\n",
    "        code,  # code to parse\n",
    "        offset_mapping,  # offset mapping from the tokenizer to align the tokens with the AST nodes\n",
    "        internal_methods,  # internal methods to parse the code\n",
    "    ):  # returns a list of AST ids and a list of parent AST ids\n",
    "        tree = self.parser.parse(bytes(code, \"utf8\"))\n",
    "        nodes = []\n",
    "        traverse(tree.root_node, nodes)\n",
    "        self.nodes = nodes\n",
    "\n",
    "        ast_nodes = []\n",
    "        for i, (start, end) in enumerate(offset_mapping):\n",
    "            ast_node = get_token_type(\n",
    "                (start, end),\n",
    "                nodes,\n",
    "                code.split(\"\\n\"),\n",
    "                internal_methods,\n",
    "                acceptable_ast_types=self.acceptable_ast_types,\n",
    "                node_types=self.node_types,\n",
    "            )\n",
    "            ast_nodes.append(ast_node)\n",
    "        return ast_nodes\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        code,  # code or list of code to tokenize\n",
    "        internal_methods=[],  # list of internal methods to check against\n",
    "        return_merged=True,  # whether to string representations of the merged ASTs and parent ASTs\n",
    "        **kwargs,  # kwargs for the underlying transformers tokenizer\n",
    "    ):  # returns a dictionary of token ids, attention masks, AST ids, parent AST ids, and optionally the string representations of the merged ASTs and parent ASTs\n",
    "        encoding = self.tokenizer(code, return_offsets_mapping=True, **kwargs)\n",
    "        encoding[\"ast_ids\"] = []\n",
    "        encoding[\"parent_ast_ids\"] = []\n",
    "        encoding[\"is_internal_methods\"] = []\n",
    "        encoding[\"is_builtins\"] = []\n",
    "        if isinstance(code, list):\n",
    "            batched_ast_nodes = []\n",
    "            if internal_methods == []:\n",
    "                internal_methods = [[] for _ in range(len(code))]\n",
    "            for i, c in enumerate(code):\n",
    "                ast_nodes = self.parse_tree(\n",
    "                    c, encoding[\"offset_mapping\"][i], internal_methods[i]\n",
    "                )\n",
    "                batched_ast_nodes.append(ast_nodes)\n",
    "                ast_ids, parent_ast_id, is_internal_methods, is_builtin = [], [], [], []\n",
    "                for ast_node in ast_nodes:\n",
    "                    if ast_node is None:\n",
    "                        ast_ids.append(-1)\n",
    "                        parent_ast_id.append(-1)\n",
    "                        is_internal_methods.append(False)\n",
    "                        is_builtin.append(False)\n",
    "                        continue\n",
    "                    ast_ids.append(ast_node.type_id)\n",
    "                    parent_ast_id.append(ast_node.parent_type_id)\n",
    "                    is_internal_methods.append(ast_node.is_internal)\n",
    "                    is_builtin.append(ast_node.is_builtin)\n",
    "                encoding[\"ast_ids\"].append(ast_ids)\n",
    "                encoding[\"parent_ast_ids\"].append(parent_ast_id)\n",
    "                encoding[\"is_internal_methods\"].append(is_internal_methods)\n",
    "                encoding[\"is_builtins\"].append(is_builtin)\n",
    "        else:\n",
    "            ast_nodes = self.parse_tree(\n",
    "                code, encoding[\"offset_mapping\"], internal_methods\n",
    "            )\n",
    "            for ast_node in ast_nodes:\n",
    "                if ast_node is None:\n",
    "                    encoding[\"ast_ids\"].append(-1)\n",
    "                    encoding[\"parent_ast_ids\"].append(-1)\n",
    "                    encoding[\"is_internal_methods\"].append(False)\n",
    "                    encoding[\"is_builtins\"].append(False)\n",
    "                    continue\n",
    "                encoding[\"ast_ids\"].append(ast_node.type_id)\n",
    "                encoding[\"parent_ast_ids\"].append(ast_node.parent_type_id)\n",
    "                encoding[\"is_internal_methods\"].append(ast_node.is_internal)\n",
    "                encoding[\"is_builtins\"].append(ast_node.is_builtin)\n",
    "\n",
    "        if return_merged:\n",
    "            # Merge the AST ids with their parent AST ids and use the names instead of the ids\n",
    "            if isinstance(code, list):\n",
    "                encoding[\"merged_ast\"] = []\n",
    "                for ast_nodes in batched_ast_nodes:\n",
    "                    merged_ast = []\n",
    "                    for ast_node in ast_nodes:\n",
    "                        merged_ast.append(\n",
    "                            str(ast_node) if ast_node is not None else \"< N/A >\"\n",
    "                        )\n",
    "                    encoding[\"merged_ast\"].append(merged_ast)\n",
    "            else:\n",
    "                encoding[\"merged_ast\"] = []\n",
    "                for ast_node in ast_nodes:\n",
    "                    encoding[\"merged_ast\"].append(\n",
    "                        str(ast_node) if ast_node is not None else \"< N/A >\"\n",
    "                    )\n",
    "\n",
    "        return encoding\n",
    "\n",
    "    def decode(self, *args, **kwargs):\n",
    "        return self.tokenizer.decode(*args, **kwargs)\n",
    "\n",
    "    @staticmethod\n",
    "    def from_pretrained(\n",
    "        name_or_path: str,  # name or path of the tokenizer\n",
    "        program_lang: str,  # language of the tokenizer\n",
    "        padding_token: str = None,  # padding token to use\n",
    "    ):  # CodeTokenizer for the given language\n",
    "        \"\"\"Create a CodeTokenizer from a pretrained tokenizer for a given language.\"\"\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(name_or_path)\n",
    "        if padding_token:\n",
    "            tokenizer.add_special_tokens({\"pad_token\": padding_token})\n",
    "\n",
    "        # Grab the node types from the tree-sitter language\n",
    "        language = Language(\n",
    "            f\"{code_tokenizers.__path__[0]}/grammars/tree-sitter-languages.so\",\n",
    "            program_lang,\n",
    "        )\n",
    "        node_path = (\n",
    "            f\"{code_tokenizers.__path__[0]}/grammars/{program_lang}/src/node-types.json\"\n",
    "        )\n",
    "        with open(node_path) as f:\n",
    "            node_types = json.load(f)\n",
    "        node_types = unroll_node_types(node_types)\n",
    "        if program_lang == \"python\":\n",
    "            node_types.append(\"as_pattern_target\")\n",
    "            node_types.append(\"ERROR\")\n",
    "\n",
    "        # Create a parser for the language\n",
    "        parser = Parser()\n",
    "        parser.set_language(language)\n",
    "\n",
    "        return CodeTokenizer(\n",
    "            tokenizer,\n",
    "            parser,\n",
    "            language,\n",
    "            node_types,\n",
    "            name_or_path,\n",
    "            program_lang,\n",
    "            padding_token,\n",
    "        )\n",
    "\n",
    "    def __reduce__(self):\n",
    "        return (\n",
    "            CodeTokenizer.from_pretrained,\n",
    "            (self.name_or_path, self.program_lang, self.padding_token),\n",
    "        )\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return (\n",
    "            self.name_or_path == other.name_or_path\n",
    "            and self.program_lang == other.program_lang\n",
    "            and self.padding_token == other.padding_token\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the tokenizer\n",
    "py_tokenizer = CodeTokenizer.from_pretrained(\"gpt2\", \"python\")\n",
    "code = \"def foo():\\n    print('hello world')\"\n",
    "\n",
    "encoding = py_tokenizer(code)\n",
    "\n",
    "assert \"ast_ids\" in encoding\n",
    "assert \"parent_ast_ids\" in encoding\n",
    "assert \"merged_ast\" in encoding\n",
    "assert len(encoding[\"ast_ids\"]) == len(encoding[\"input_ids\"])\n",
    "assert len(encoding[\"parent_ast_ids\"]) == len(encoding[\"input_ids\"])\n",
    "assert len(encoding[\"merged_ast\"]) == len(encoding[\"input_ids\"])\n",
    "assert len(encoding[\"is_internal_methods\"]) == len(encoding[\"input_ids\"])\n",
    "assert len(encoding[\"is_builtins\"]) == len(encoding[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test with list of code\n",
    "code = [\"def foo():\\n    print('hello world')\", \"def bar():\\n    print('hello world')\"]\n",
    "encoding = py_tokenizer(code)\n",
    "\n",
    "assert \"ast_ids\" in encoding\n",
    "assert \"parent_ast_ids\" in encoding\n",
    "assert \"merged_ast\" in encoding\n",
    "assert len(encoding[\"ast_ids\"]) == len(encoding[\"input_ids\"])\n",
    "assert len(encoding[\"parent_ast_ids\"]) == len(encoding[\"input_ids\"])\n",
    "assert len(encoding[\"merged_ast\"]) == len(encoding[\"input_ids\"])\n",
    "assert len(encoding[\"is_internal_methods\"]) == len(encoding[\"input_ids\"])\n",
    "assert len(encoding[\"is_builtins\"]) == len(encoding[\"input_ids\"])\n",
    "assert len(encoding[\"ast_ids\"][0]) == len(encoding[\"input_ids\"][0])\n",
    "assert len(encoding[\"parent_ast_ids\"][0]) == len(encoding[\"input_ids\"][0])\n",
    "assert len(encoding[\"merged_ast\"][0]) == len(encoding[\"input_ids\"][0])\n",
    "assert len(encoding[\"is_internal_methods\"][0]) == len(encoding[\"input_ids\"][0])\n",
    "assert len(encoding[\"is_builtins\"][0]) == len(encoding[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test with internal methods\n",
    "code = \"def print():\\n    print('print') #print\\n    print = 1\"\n",
    "encoding = py_tokenizer(code, internal_methods=[\"print\"])\n",
    "\n",
    "for i in range(len(encoding[\"input_ids\"])):\n",
    "    if (\n",
    "        \"call\" in encoding[\"merged_ast\"][i]\n",
    "        or \"argument_list\" in encoding[\"merged_ast\"][i]\n",
    "    ):\n",
    "        assert encoding[\"is_internal_methods\"][i] == True, encoding[\"merged_ast\"][i]\n",
    "    else:\n",
    "        assert encoding[\"is_internal_methods\"][i] == False, encoding[\"merged_ast\"][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test with internal methods and batched\n",
    "code = \"def foo():\\n    print('print') #print\"\n",
    "encoding = py_tokenizer([code] * 2, internal_methods=[[\"print\"], [\"print\"]])\n",
    "\n",
    "for i in range(len(encoding[\"input_ids\"])):\n",
    "    for j in range(len(encoding[\"input_ids\"][i])):\n",
    "        if (\n",
    "            \"call\" in encoding[\"merged_ast\"][i][j]\n",
    "            or \"argument_list\" in encoding[\"merged_ast\"][i][j]\n",
    "        ):\n",
    "            assert encoding[\"is_internal_methods\"][i][j] == True, encoding[\n",
    "                \"merged_ast\"\n",
    "            ][i][j]\n",
    "        else:\n",
    "            assert encoding[\"is_internal_methods\"][i][j] == False, encoding[\n",
    "                \"merged_ast\"\n",
    "            ][i][j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test without internal methods\n",
    "code = \"def foo():\\n    print('print') #print\"\n",
    "encoding = py_tokenizer(code)\n",
    "\n",
    "for i in range(len(encoding[\"input_ids\"])):\n",
    "    assert encoding[\"is_internal_methods\"][i] == False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test without internal methods and batched\n",
    "code = \"def foo():\\n    print('print') #print\"\n",
    "encoding = py_tokenizer([code] * 2)\n",
    "\n",
    "for i in range(len(encoding[\"input_ids\"])):\n",
    "    for j in range(len(encoding[\"input_ids\"][i])):\n",
    "        assert encoding[\"is_internal_methods\"][i][j] == False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test with builtins\n",
    "code = \"def foo():\\n    print('print') #print\\n    print = 1\"\n",
    "encoding = py_tokenizer(code)\n",
    "\n",
    "for i in range(len(encoding[\"input_ids\"])):\n",
    "    if \"call\" in encoding[\"merged_ast\"][i]:\n",
    "        assert encoding[\"is_builtins\"][i] == True, encoding[\"merged_ast\"][i]\n",
    "    else:\n",
    "        assert encoding[\"is_builtins\"][i] == False, encoding[\"merged_ast\"][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test with builtins and batched\n",
    "code = \"def foo():\\n    print('print') #print\"\n",
    "encoding = py_tokenizer([code] * 2)\n",
    "\n",
    "for i in range(len(encoding[\"input_ids\"])):\n",
    "    for j in range(len(encoding[\"input_ids\"][i])):\n",
    "        if \"call\" in encoding[\"merged_ast\"][i][j]:\n",
    "            assert encoding[\"is_builtins\"][i][j] == True, encoding[\"merged_ast\"][i][j]\n",
    "        else:\n",
    "            assert encoding[\"is_builtins\"][i][j] == False, encoding[\"merged_ast\"][i][j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |eval: false\n",
    "# test the pickleability of the tokenizer\n",
    "import pickle\n",
    "\n",
    "assert py_tokenizer == pickle.loads(pickle.dumps(py_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf36473178a548998b7a0552c940179f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/401 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration codeparrot--codeparrot-clean-valid-826c6fd8b27e5523\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/codeparrot--codeparrot-clean-valid to /work/.cache/huggingface/datasets/codeparrot___json/codeparrot--codeparrot-clean-valid-826c6fd8b27e5523/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bdd18853ada4e61814e9274336d1349",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4500f777af6743b49612266e5261639a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/142M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a81f26272e084a0b81ea5d5f3eb25c18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f4618e4c62446e2a6387a297bf2bb57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /work/.cache/huggingface/datasets/codeparrot___json/codeparrot--codeparrot-clean-valid-826c6fd8b27e5523/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ad7947cc93240cb9ea66b9e5cea5f70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1185 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec4da43b2dbe4fdf9421dd06eea894dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/3 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db769132661540c9bb9d3d9c809f9b4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/3 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfa357913b9d46a38ad86b1b580a9217",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/2 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "331537c6ced7443385705c41450a2c52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/2 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# |eval: false\n",
    "# test the time of multi-proc tokenization is faster than single proc tokenization\n",
    "import time\n",
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"codeparrot/codeparrot-clean-valid\", split=\"train\").select(range(10))\n",
    "\n",
    "start = time.time()\n",
    "single_proc_ds = ds.map(\n",
    "    lambda x: py_tokenizer(x[\"content\"]),\n",
    "    batched=False,\n",
    "    batch_size=1,\n",
    "    num_proc=1,\n",
    "    load_from_cache_file=False,\n",
    ")\n",
    "total_single_proc = time.time() - start\n",
    "\n",
    "start = time.time()\n",
    "multi_proc_ds = ds.map(\n",
    "    lambda x: py_tokenizer(x[\"content\"]),\n",
    "    batched=False,\n",
    "    batch_size=1,\n",
    "    num_proc=4,\n",
    "    load_from_cache_file=False,\n",
    ")\n",
    "total_multi_proc = time.time() - start\n",
    "\n",
    "assert total_multi_proc < total_single_proc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |eval: false\n",
    "# test that the two datasets tokenized with single and multi processing are identical\n",
    "\n",
    "for i in range(len(ds)):\n",
    "    assert single_proc_ds[i][\"input_ids\"] == multi_proc_ds[i][\"input_ids\"]\n",
    "    assert single_proc_ds[i][\"attention_mask\"] == multi_proc_ds[i][\"attention_mask\"]\n",
    "    assert single_proc_ds[i][\"offset_mapping\"] == multi_proc_ds[i][\"offset_mapping\"]\n",
    "    assert single_proc_ds[i][\"ast_ids\"] == multi_proc_ds[i][\"ast_ids\"]\n",
    "    assert single_proc_ds[i][\"parent_ast_ids\"] == multi_proc_ds[i][\"parent_ast_ids\"]\n",
    "    assert single_proc_ds[i][\"merged_ast\"] == multi_proc_ds[i][\"merged_ast\"]\n",
    "    assert (\n",
    "        single_proc_ds[i][\"is_internal_methods\"]\n",
    "        == multi_proc_ds[i][\"is_internal_methods\"]\n",
    "    )\n",
    "    assert single_proc_ds[i][\"is_builtins\"] == multi_proc_ds[i][\"is_builtins\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "You must install black: `pip install black` if you wish to use black formatting with nbdev",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/code_tokenizers/lib/python3.10/site-packages/nbdev/export.py:36\u001b[0m, in \u001b[0;36mblack_format\u001b[0;34m(cell, force)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mnot\u001b[39;00m cfg\u001b[39m.\u001b[39mblack_formatting \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m force) \u001b[39mor\u001b[39;00m cell\u001b[39m.\u001b[39mcell_type \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcode\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m \u001b[39mtry\u001b[39;00m: \u001b[39mimport\u001b[39;00m \u001b[39mblack\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[39mexcept\u001b[39;00m: \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mYou must install black: `pip install black` if you wish to use black formatting with nbdev\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'black'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [27], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m#| hide\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnbdev\u001b[39;00m; nbdev\u001b[39m.\u001b[39;49mnbdev_export()\n",
      "File \u001b[0;32m~/miniconda3/envs/code_tokenizers/lib/python3.10/site-packages/fastcore/script.py:110\u001b[0m, in \u001b[0;36mcall_parse.<locals>._f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[39m@wraps\u001b[39m(func)\n\u001b[1;32m    108\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_f\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    109\u001b[0m     mod \u001b[39m=\u001b[39m inspect\u001b[39m.\u001b[39mgetmodule(inspect\u001b[39m.\u001b[39mcurrentframe()\u001b[39m.\u001b[39mf_back)\n\u001b[0;32m--> 110\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m mod: \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    111\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m SCRIPT_INFO\u001b[39m.\u001b[39mfunc \u001b[39mand\u001b[39;00m mod\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m==\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m: SCRIPT_INFO\u001b[39m.\u001b[39mfunc \u001b[39m=\u001b[39m func\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\n\u001b[1;32m    112\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(sys\u001b[39m.\u001b[39margv)\u001b[39m>\u001b[39m\u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m sys\u001b[39m.\u001b[39margv[\u001b[39m1\u001b[39m]\u001b[39m==\u001b[39m\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m: sys\u001b[39m.\u001b[39margv\u001b[39m.\u001b[39mpop(\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/code_tokenizers/lib/python3.10/site-packages/nbdev/doclinks.py:137\u001b[0m, in \u001b[0;36mnbdev_export\u001b[0;34m(path, **kwargs)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39menviron\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mIN_TEST\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m0\u001b[39m): \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    136\u001b[0m files \u001b[39m=\u001b[39m nbglob(path\u001b[39m=\u001b[39mpath, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 137\u001b[0m \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m files: nb_export(f)\n\u001b[1;32m    138\u001b[0m add_init(get_config()\u001b[39m.\u001b[39mlib_path)\n\u001b[1;32m    139\u001b[0m _build_modidx()\n",
      "File \u001b[0;32m~/miniconda3/envs/code_tokenizers/lib/python3.10/site-packages/nbdev/export.py:49\u001b[0m, in \u001b[0;36mnb_export\u001b[0;34m(nbname, lib_path, procs, debug, mod_maker, name)\u001b[0m\n\u001b[1;32m     47\u001b[0m exp \u001b[39m=\u001b[39m ExportModuleProc()\n\u001b[1;32m     48\u001b[0m nb \u001b[39m=\u001b[39m NBProcessor(nbname, [exp]\u001b[39m+\u001b[39mL(procs), debug\u001b[39m=\u001b[39mdebug)\n\u001b[0;32m---> 49\u001b[0m nb\u001b[39m.\u001b[39;49mprocess()\n\u001b[1;32m     50\u001b[0m \u001b[39mfor\u001b[39;00m mod,cells \u001b[39min\u001b[39;00m exp\u001b[39m.\u001b[39mmodules\u001b[39m.\u001b[39mitems():\n\u001b[1;32m     51\u001b[0m     all_cells \u001b[39m=\u001b[39m exp\u001b[39m.\u001b[39min_all[mod]\n",
      "File \u001b[0;32m~/miniconda3/envs/code_tokenizers/lib/python3.10/site-packages/nbdev/process.py:126\u001b[0m, in \u001b[0;36mNBProcessor.process\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprocess\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    125\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mProcess all cells with all processors\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 126\u001b[0m     \u001b[39mfor\u001b[39;00m proc \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocs: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_proc(proc)\n",
      "File \u001b[0;32m~/miniconda3/envs/code_tokenizers/lib/python3.10/site-packages/nbdev/process.py:119\u001b[0m, in \u001b[0;36mNBProcessor._proc\u001b[0;34m(self, proc)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_proc\u001b[39m(\u001b[39mself\u001b[39m, proc):\n\u001b[1;32m    118\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(proc,\u001b[39m'\u001b[39m\u001b[39mbegin\u001b[39m\u001b[39m'\u001b[39m): proc\u001b[39m.\u001b[39mbegin()\n\u001b[0;32m--> 119\u001b[0m     \u001b[39mfor\u001b[39;00m cell \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnb\u001b[39m.\u001b[39mcells: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process_cell(proc, cell)\n\u001b[1;32m    120\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(proc,\u001b[39m'\u001b[39m\u001b[39mend\u001b[39m\u001b[39m'\u001b[39m): proc\u001b[39m.\u001b[39mend()\n\u001b[1;32m    121\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnb\u001b[39m.\u001b[39mcells \u001b[39m=\u001b[39m [c \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnb\u001b[39m.\u001b[39mcells \u001b[39mif\u001b[39;00m c \u001b[39mand\u001b[39;00m \u001b[39mgetattr\u001b[39m(c,\u001b[39m'\u001b[39m\u001b[39msource\u001b[39m\u001b[39m'\u001b[39m,\u001b[39mNone\u001b[39;00m) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m]\n",
      "File \u001b[0;32m~/miniconda3/envs/code_tokenizers/lib/python3.10/site-packages/nbdev/process.py:110\u001b[0m, in \u001b[0;36mNBProcessor._process_cell\u001b[0;34m(self, proc, cell)\u001b[0m\n\u001b[1;32m    108\u001b[0m         f \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(proc, \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00mcmd\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    109\u001b[0m         \u001b[39mif\u001b[39;00m f: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_comment(f, cell, cmd)\n\u001b[0;32m--> 110\u001b[0m \u001b[39mif\u001b[39;00m callable(proc) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m _is_direc(proc): cell \u001b[39m=\u001b[39m opt_set(cell, proc(cell))\n",
      "File \u001b[0;32m~/miniconda3/envs/code_tokenizers/lib/python3.10/site-packages/nbdev/export.py:37\u001b[0m, in \u001b[0;36mblack_format\u001b[0;34m(cell, force)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mnot\u001b[39;00m cfg\u001b[39m.\u001b[39mblack_formatting \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m force) \u001b[39mor\u001b[39;00m cell\u001b[39m.\u001b[39mcell_type \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcode\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[39mtry\u001b[39;00m: \u001b[39mimport\u001b[39;00m \u001b[39mblack\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m \u001b[39mexcept\u001b[39;00m: \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mYou must install black: `pip install black` if you wish to use black formatting with nbdev\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     38\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m     _format_str \u001b[39m=\u001b[39m partial(black\u001b[39m.\u001b[39mformat_str, mode \u001b[39m=\u001b[39m black\u001b[39m.\u001b[39mMode())\n",
      "\u001b[0;31mImportError\u001b[0m: You must install black: `pip install black` if you wish to use black formatting with nbdev"
     ]
    }
   ],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "code_tokenizers",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
