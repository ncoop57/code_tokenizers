{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# core\n",
    "\n",
    "> Fill in a module description here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import code_tokenizers\n",
    "import json\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from tree_sitter import Language, Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def unroll_node_types(\n",
    "    nested_node_types: dict, # node_types from tree-sitter\n",
    ") -> list: # list of node types\n",
    "    \"\"\"Unroll nested node types into a flat list of node types. This includes subtypes as well.\"\"\"\n",
    "    node_types = [node_type[\"type\"] for node_type in nested_node_types]\n",
    "    node_subtypes = [\n",
    "        node_subtype[\"type\"]\n",
    "        for node_type in node_types\n",
    "        if \"subtypes\" in node_type\n",
    "        for node_subtype in node_type[\"subtypes\"]\n",
    "    ]\n",
    "    return list(set(node_types + node_subtypes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# From: https://github.com/github/CodeSearchNet/tree/master/function_parser\n",
    "def traverse(\n",
    "    node,       # tree-sitter node\n",
    "    results,    # list to append results to\n",
    ") -> None:\n",
    "    \"\"\"Traverse in a recursive way, a tree-sitter node and append results to a list.\"\"\"\n",
    "    if node.type == 'string':\n",
    "        results.append(node)\n",
    "        return\n",
    "    for n in node.children:\n",
    "        traverse(n, results)\n",
    "    if not node.children:\n",
    "        results.append(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_token_type(\n",
    "    tok_span: tuple, # (start, end) position of a token\n",
    "    nodes: list,     # list of tree-sitter nodes\n",
    "    lines: list,     # list of lines in the code\n",
    ") -> tuple: # (parent_type, token_type) of the token\n",
    "    \"\"\"Get the parent AST type and token AST type of a token.\"\"\"\n",
    "    def get_node_span(node):\n",
    "        def convert_to_offset(point):\n",
    "            row, column = point\n",
    "            chars_in_rows = sum(map(len, lines[:row])) + row\n",
    "            chars_in_columns = len(lines[row][:column])\n",
    "\n",
    "            offset = chars_in_rows + chars_in_columns\n",
    "            return offset\n",
    "        start_span = convert_to_offset(node.start_point)\n",
    "        end_span = convert_to_offset(node.end_point)\n",
    "        return start_span, end_span\n",
    "    \n",
    "    node_spans = [get_node_span(node) for node in nodes]\n",
    "    for i, span in enumerate(node_spans):\n",
    "        if (span[0] <= tok_span[0] and tok_span[0] < span[1]) or (span[0] < tok_span[1] and tok_span[1] <= span[1]):\n",
    "            return nodes[i].parent.type, nodes[i].type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class CodeTokenizer():\n",
    "    \"\"\"A tokenizer for code, which aligns the tokens with the AST nodes.\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer,  # transformers tokenizer\n",
    "        parser,     # tree-sitter parser\n",
    "        node_types  # list of node types\n",
    "    ):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.parser = parser\n",
    "        self.node_types = node_types\n",
    "    \n",
    "    def parse_tree(\n",
    "        self,\n",
    "        code,           # code to parse\n",
    "        offset_mapping  # offset mapping from the tokenizer to align the tokens with the AST nodes\n",
    "    ):                  # returns a list of AST ids and a list of parent AST ids\n",
    "        tree = self.parser.parse(bytes(code, \"utf8\"))\n",
    "        nodes = []\n",
    "        traverse(tree.root_node, nodes)\n",
    "\n",
    "        ast_ids = []\n",
    "        parent_ast_ids = []\n",
    "        for i, (start, end) in enumerate(offset_mapping):\n",
    "            if start == None or end == None:\n",
    "                ast_ids.append(-1)\n",
    "                parent_ast_ids.append(-1)\n",
    "                continue\n",
    "            type_info = get_token_type((start, end), nodes, code.split(\"\\n\"))\n",
    "            if type_info is None:\n",
    "                ast_ids.append(-1)\n",
    "                parent_ast_ids.append(-1)\n",
    "            else:\n",
    "                parent_node_type, node_type = type_info\n",
    "                try:\n",
    "                    ast_ids.append(self.node_types.index(node_type))\n",
    "                    parent_ast_ids.append(self.node_types.index(parent_node_type))\n",
    "                except Exception as e:\n",
    "                    print(type_info)\n",
    "                    print(code)\n",
    "                    ast_ids.append(-1)\n",
    "                    parent_ast_ids.append(-1)\n",
    "                    raise e\n",
    "\n",
    "        return ast_ids, parent_ast_ids\n",
    "    \n",
    "    def __call__(\n",
    "        self,\n",
    "        code,               # code or list of code to tokenize\n",
    "        return_merged=True, # whether to string representations of the merged ASTs and parent ASTs\n",
    "        **kwargs            # kwargs for the underlying transformers tokenizer\n",
    "    ):                      # returns a dictionary of token ids, attention masks, AST ids, parent AST ids, and optionally the string representations of the merged ASTs and parent ASTs\n",
    "        encoding = self.tokenizer(code, return_offsets_mapping=True, **kwargs)\n",
    "        if isinstance(code, list):\n",
    "            encoding[\"ast_ids\"] = []\n",
    "            encoding[\"parent_ast_ids\"] = []\n",
    "            for i, c in enumerate(code):\n",
    "                ast_ids, parent_ast_ids = self.parse_tree(c, encoding[\"offset_mapping\"][i])\n",
    "                encoding[\"ast_ids\"].append(ast_ids)\n",
    "                encoding[\"parent_ast_ids\"].append(parent_ast_ids)\n",
    "        else:\n",
    "            encoding[\"ast_ids\"], encoding[\"parent_ast_ids\"] = self.parse_tree(code, encoding[\"offset_mapping\"])\n",
    "        \n",
    "        if return_merged:\n",
    "            # Merge the AST ids with their parent AST ids and use the names instead of the ids\n",
    "            if isinstance(code, list):\n",
    "                encoding[\"merged_ast\"] = []\n",
    "                for parent_ast_ids, ast_ids in zip(encoding[\"parent_ast_ids\"], encoding[\"ast_ids\"]):\n",
    "                    merged_ast = []\n",
    "                    for i, (parent_ast_id, ast_id) in enumerate(zip(parent_ast_ids, ast_ids)):\n",
    "                        if parent_ast_id == -1 or ast_id == -1:\n",
    "                            merged_ast.append(\"< N/A >\")\n",
    "                        else:\n",
    "                            merged_ast.append(\n",
    "                                f\"<{self.node_types[parent_ast_id]} -> {self.node_types[ast_id]}>\"\n",
    "                            )\n",
    "\n",
    "                    encoding[\"merged_ast\"].append(merged_ast)\n",
    "            else:\n",
    "                encoding[\"merged_ast\"] = []\n",
    "                for parent_ast_id, ast_id in zip(encoding[\"parent_ast_ids\"], encoding[\"ast_ids\"]):\n",
    "                    if parent_ast_id == -1 or ast_id == -1:\n",
    "                        encoding[\"merged_ast\"].append(\"< N/A >\")\n",
    "                    else:\n",
    "                        encoding[\"merged_ast\"].append(\n",
    "                            f\"<{self.node_types[parent_ast_id]} -> {self.node_types[ast_id]}>\"\n",
    "                        )\n",
    "\n",
    "        return encoding\n",
    "    \n",
    "    def decode(self, *args, **kwargs):\n",
    "        return self.tokenizer.decode(*args, **kwargs)\n",
    "\n",
    "    @staticmethod\n",
    "    def from_pretrained(\n",
    "        name_or_path: str,  # name or path of the tokenizer\n",
    "        lang: str,          # language of the tokenizer\n",
    "    ):                      # CodeTokenizer for the given language\n",
    "        \"\"\"Create a CodeTokenizer from a pretrained tokenizer for a given language.\"\"\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(name_or_path)\n",
    "\n",
    "        # Grab the node types from the tree-sitter language\n",
    "        language = Language(f\"{code_tokenizers.__path__[0]}/grammars/tree-sitter-languages.so\", lang)\n",
    "        node_path = f\"{code_tokenizers.__path__[0]}/grammars/tree-sitter-{lang}/src/node-types.json\"\n",
    "        with open(node_path) as f:\n",
    "            node_types = json.load(f)\n",
    "        node_types = unroll_node_types(node_types)\n",
    "        if lang == \"python\":\n",
    "            node_types.append(\"as_pattern_target\")\n",
    "\n",
    "        # Create a parser for the language\n",
    "        parser = Parser()\n",
    "        parser.set_language(language)\n",
    "        \n",
    "        return CodeTokenizer(tokenizer, parser, node_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the tokenizer\n",
    "py_tokenizer = CodeTokenizer.from_pretrained(\"gpt2\", \"python\")\n",
    "code = \"def foo():\\n    print('hello world')\"\n",
    "\n",
    "encoding = py_tokenizer(code)\n",
    "\n",
    "assert \"ast_ids\" in encoding\n",
    "assert \"parent_ast_ids\" in encoding\n",
    "assert \"merged_ast\" in encoding\n",
    "assert len(encoding[\"ast_ids\"]) == len(encoding[\"input_ids\"])\n",
    "assert len(encoding[\"parent_ast_ids\"]) == len(encoding[\"input_ids\"])\n",
    "assert len(encoding[\"merged_ast\"]) == len(encoding[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test with list of code\n",
    "code = [\"def foo():\\n    print('hello world')\", \"def bar():\\n    print('hello world')\"]\n",
    "encoding = py_tokenizer(code)\n",
    "\n",
    "assert \"ast_ids\" in encoding\n",
    "assert \"parent_ast_ids\" in encoding\n",
    "assert \"merged_ast\" in encoding\n",
    "assert len(encoding[\"ast_ids\"]) == len(encoding[\"input_ids\"])\n",
    "assert len(encoding[\"parent_ast_ids\"]) == len(encoding[\"input_ids\"])\n",
    "assert len(encoding[\"merged_ast\"]) == len(encoding[\"input_ids\"])\n",
    "assert len(encoding[\"ast_ids\"][0]) == len(encoding[\"input_ids\"][0])\n",
    "assert len(encoding[\"parent_ast_ids\"][0]) == len(encoding[\"input_ids\"][0])\n",
    "assert len(encoding[\"merged_ast\"][0]) == len(encoding[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('code_tokenizers')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
